import json
import time
from pathlib import Path
from typing import Any, Dict, List, Optional
from dataclasses import dataclass, asdict
import logging

logger = logging.getLogger(__name__)


@dataclass
class RetrievalResult:
    """å­˜å‚¨å•æ¬¡æ£€ç´¢çš„å®Œæ•´ç»“æœ"""
    
    # åŸºæœ¬ä¿¡æ¯
    query: str
    query_mode: str  # local, global, hybrid, mix, naive
    timestamp: float
    
    # æ£€ç´¢åˆ°çš„å†…å®¹
    entities: List[Dict[str, Any]]
    relationships: List[Dict[str, Any]]
    text_chunks: List[Dict[str, Any]]
    
    # å…³é”®è¯ä¿¡æ¯
    high_level_keywords: List[str]
    low_level_keywords: List[str]
    
    # æ£€ç´¢å…ƒæ•°æ®
    metadata: Dict[str, Any]
    
    # æœ€ç»ˆå›ç­”ï¼ˆå¯é€‰ï¼‰
    final_response: Optional[str] = None


class RetrievalLogger:
    """æ£€ç´¢ç»“æœæ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, log_dir: str = "./retrieval_logs"):
        """
        åˆå§‹åŒ–æ£€ç´¢æ—¥å¿—è®°å½•å™¨
        
        Args:
            log_dir: æ—¥å¿—å­˜å‚¨ç›®å½•
        """
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå½“å‰ä¼šè¯çš„æ—¥å¿—æ–‡ä»¶
        self.session_id = int(time.time())
        self.log_file = self.log_dir / f"retrieval_log_{self.session_id}.jsonl"
        
        logger.info(f"RetrievalLogger initialized. Log file: {self.log_file}")
    
    def log_retrieval(self, retrieval_result: RetrievalResult) -> None:
        """
        è®°å½•å•æ¬¡æ£€ç´¢ç»“æœåˆ°JSONLæ–‡ä»¶
        
        Args:
            retrieval_result: æ£€ç´¢ç»“æœå¯¹è±¡
        """
        try:
            # è½¬æ¢ä¸ºå­—å…¸
            result_dict = asdict(retrieval_result)
            
            # å†™å…¥JSONLæ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
            with open(self.log_file, 'a', encoding='utf-8') as f:
                json.dump(result_dict, f, ensure_ascii=False)
                f.write('\n')
            
            logger.debug(f"Logged retrieval result for query: {retrieval_result.query[:50]}...")
            
        except Exception as e:
            logger.error(f"Failed to log retrieval result: {e}")
    
    def load_logs(self, log_file: Optional[str] = None) -> List[RetrievalResult]:
        """
        åŠ è½½æ—¥å¿—æ–‡ä»¶ä¸­çš„æ£€ç´¢ç»“æœ
        
        Args:
            log_file: æŒ‡å®šæ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ŒNoneåˆ™ä½¿ç”¨å½“å‰ä¼šè¯çš„æ—¥å¿—
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        file_path = Path(log_file) if log_file else self.log_file
        
        if not file_path.exists():
            logger.warning(f"Log file not found: {file_path}")
            return []
        
        results = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        result_dict = json.loads(line)
                        results.append(RetrievalResult(**result_dict))
            
            logger.info(f"Loaded {len(results)} retrieval results from {file_path}")
            return results
            
        except Exception as e:
            logger.error(f"Failed to load logs from {file_path}: {e}")
            return []
    def export_to_json(self, output_file: str, log_file: Optional[str] = None) -> None:
        """
        å¯¼å‡ºæ—¥å¿—ä¸ºå•ä¸ªJSONæ–‡ä»¶ï¼ˆä¾¿äºåˆ†æï¼‰
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            log_file: æºæ—¥å¿—æ–‡ä»¶ï¼ŒNoneåˆ™å¯¼å‡ºæ•´ä¸ªæ—¥å¿—ç›®å½•çš„æ‰€æœ‰æ–‡ä»¶
        """
        # ğŸ”¥ æ–°å¢ï¼šå¦‚æœæ²¡æœ‰æŒ‡å®š log_fileï¼Œåˆ™è¯»å–æ•´ä¸ªç›®å½•çš„æ‰€æœ‰ JSONL æ–‡ä»¶
        if log_file is None:
            all_results = []
            
            # è·å–ç›®å½•ä¸­æ‰€æœ‰ .jsonl æ–‡ä»¶
            jsonl_files = sorted(self.log_dir.glob("retrieval_log_*.jsonl"))
            
            if not jsonl_files:
                logger.warning(f"No JSONL files found in {self.log_dir}")
                return
            
            logger.info(f"Found {len(jsonl_files)} JSONL files to export")
            
            # ä¾æ¬¡è¯»å–æ¯ä¸ª JSONL æ–‡ä»¶
            for jsonl_file in jsonl_files:
                logger.info(f"Reading {jsonl_file.name}...")
                results = self.load_logs(str(jsonl_file))
                all_results.extend(results)
            
            logger.info(f"Total loaded: {len(all_results)} retrieval results")
            
        else:
            # å¦‚æœæŒ‡å®šäº† log_fileï¼Œåªè¯»å–è¯¥æ–‡ä»¶ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰
            all_results = self.load_logs(log_file)
        
        try:
            # ğŸ”¥ ç¡®ä¿è¾“å‡ºæ–‡ä»¶çš„çˆ¶ç›®å½•å­˜åœ¨
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump([asdict(r) for r in all_results], f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… Exported {len(all_results)} results to {output_file}")
            
        except Exception as e:
            logger.error(f"Failed to export to {output_file}: {e}")    
    
    def get_statistics(self, log_file: Optional[str] = None) -> Dict[str, Any]:
        """
        è·å–æ£€ç´¢æ—¥å¿—çš„ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            log_file: æŒ‡å®šæ—¥å¿—æ–‡ä»¶ï¼ŒNoneåˆ™ä½¿ç”¨å½“å‰ä¼šè¯çš„æ—¥å¿—
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        results = self.load_logs(log_file)
        
        if not results:
            return {}
        
        stats = {
            "total_queries": len(results),
            "mode_distribution": {},
            "avg_entities_per_query": 0,
            "avg_relationships_per_query": 0,
            "avg_chunks_per_query": 0,
            "queries_with_no_results": 0,
        }
        
        total_entities = 0
        total_relationships = 0
        total_chunks = 0
        
        for result in results:
            # æ¨¡å¼åˆ†å¸ƒ
            mode = result.query_mode
            stats["mode_distribution"][mode] = stats["mode_distribution"].get(mode, 0) + 1
            
            # ç´¯è®¡æ•°é‡
            total_entities += len(result.entities)
            total_relationships += len(result.relationships)
            total_chunks += len(result.text_chunks)
            
            # ç©ºç»“æœæŸ¥è¯¢
            if (not result.entities and not result.relationships and not result.text_chunks):
                stats["queries_with_no_results"] += 1
        
        # è®¡ç®—å¹³å‡å€¼
        stats["avg_entities_per_query"] = total_entities / len(results)
        stats["avg_relationships_per_query"] = total_relationships / len(results)
        stats["avg_chunks_per_query"] = total_chunks / len(results)
        
        return stats